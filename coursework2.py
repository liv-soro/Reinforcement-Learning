# -*- coding: utf-8 -*-
"""coursework2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qlhL1tHGXxfQlLYtykUAgl7SVlwGk8fe

# Set-Up
"""

# This is the coursework 2 for the Reinforcement Leaning course 2021 taught at Imperial College London (https://www.imperial.ac.uk/computing/current-students/courses/70028/)
# The code is based on the OpenAI Gym original (https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) and modified by Filippo Valdettaro and Prof. Aldo Faisal for the purposes of the course.
# There may be differences to the reference implementation in OpenAI gym and other solutions floating on the internet, but this is the defeinitive implementation for the course.

# Instaling in Google Colab the libraries used for the coursework
# You do NOT need to understand it to work on this coursework

# WARNING: if you don't use this Notebook in Google Colab, this block might print some warnings (do not mind them)

!pip install gym pyvirtualdisplay > /dev/null 2>&1
!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1
!pip install colabgymrender==1.0.2
!wget http://www.atarimania.com/roms/Roms.rar
!mkdir /content/ROM/
!unrar e /content/Roms.rar /content/ROM/
!python -m atari_py.import_roms /content/ROM/

from IPython.display import clear_output
clear_output()

# Importing the libraries

import gym
from gym.wrappers.monitoring.video_recorder import VideoRecorder    #records videos of episodes
import numpy as np
import matplotlib.pyplot as plt # Graphical library

import torch
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F
device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # Configuring Pytorch

from collections import namedtuple, deque
from itertools import count
import math
import random

# WARNING: if you don't use this Notebook in Google Colab, comment out these two imports
from colabgymrender.recorder import Recorder # Allow to record videos in Google Colab
Recorder(gym.make("CartPole-v1"), './video') # Defining the video recorder
clear_output()

# Test cell: check ai gym  environment + recording working as intended

env = gym.make("CartPole-v1")
file_path = 'video/video.mp4'
recorder = VideoRecorder(env, file_path)

observation = env.reset()
terminal = False
while not terminal:
  recorder.capture_frame()
  action = int(observation[2]>0) # Angle > 0, push to the right, angle < 0, push to the left
  observation, reward, terminal, info = env.step(action)
  # Observation is position, velocity, angle, angular velocity

recorder.close()
env.close()

from google.colab import drive
drive.mount('/content/gdrive')
PATH = 'gdrive/MyDrive/Reinforcement Learning/'

import pickle

"""# Functions and Classes (multipurpose)"""

Transition = namedtuple('Transition',
                        ('state', 'action', 'next_state', 'reward'))


class ReplayBuffer(object): #we'll have objects of class ReplayBuffer

    
    def __init__(self, capacity):
      self.memory = deque([], maxlen = capacity) # Deque: double ended queues
      # Once a bounded length deque is full, when new items are added, 
      # a corresponding number of items are discarded from the opposite end

    def push(self, *args):
      """Save a transition (state, action, next_state, reward)"""
      self.memory.append(Transition(*args))

    def sample(self, batch_size):
      return random.sample(self.memory, batch_size)

    def __len__(self):
      return len(self.memory)

class DQN(nn.Module):

    def __init__(self, inputs, outputs, num_hidden, hidden_size):
      """
      Initialise the network.
      Input:
          - inputs {int} - size of input to the network
          - outputs {int} - size of output to the network
          - num_hidden {int} - number of hidden layers
          - hidden_size {int} - size of each hidden layer
      """
      super(DQN, self).__init__()
      self.input_layer = nn.Linear(inputs, hidden_size)
      self.hidden_layers = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(num_hidden-1)])
      self.output_layer = nn.Linear(hidden_size, outputs)
    
    def forward(self, x):
      """
      Get the output of the DQN.
      Input: x {tensor} - one element or a batch of elements
      Ouput: y {tensor} - corresponding output
      """
      x.to(device)

      x = F.relu(self.input_layer(x))
      for layer in self.hidden_layers:
          x = F.relu(layer(x)) # Passing through each hidden layer and applying Relu activation
      
      return self.output_layer(x) # Return after passing thorugh the output layer

def optimize_model():
  if len(memory) < BATCH_SIZE:
    return
  transitions = memory.sample(BATCH_SIZE)
  batch = Transition(*zip(*transitions)) 
  
  # Compute a mask of non-final states and concatenate the batch elements
  non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,
                                        batch.next_state)), device=device, dtype=torch.bool) # tensor of boolean values (True = non-final state)
  if sum(non_final_mask) > 0:
    non_final_next_states = torch.cat([s for s in batch.next_state
                                                    if s is not None])
  else:
    non_final_next_states = torch.empty(0,state_dim).to(device)

  state_batch = torch.cat(batch.state) # tensor with all the states in the batch
  action_batch = torch.cat(batch.action) # tensor with all the actions in the batch
  reward_batch = torch.cat(batch.reward) # tensor with all the rewards in the batch
  
  # Compute Q(s_t, a) 
  state_action_values = policy_net(state_batch).gather(1, action_batch) # Compute Q for all possible actions on all states in the batch (state_batch) and choose the one for the action taken (with .gather())
  
  # Compute V(s_{t+1}) for all next states.
  next_state_values = torch.zeros(BATCH_SIZE, device=device)
  with torch.no_grad():
    if sum(non_final_mask) > 0:
      next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach() # I want to select the action that give the max Q (return this Q). Detach() is to detach the gradient (don't want to store).
    else:
      next_state_values = torch.zeros_like(next_state_values)

  # Compute the expected Q values  
  expected_state_action_values = (next_state_values * GAMMA) + reward_batch

  # Compute loss
  loss = ((state_action_values - expected_state_action_values.unsqueeze(1))**2).sum()
  
  # Optimise the model
  optimizer.zero_grad()
  loss.backward()
  
  # Limit magnitude of gradient for update step
  for param in policy_net.parameters():
      param.grad.data.clamp_(-1, 1)
  optimizer.step()

def select_action(state, current_eps=0):
    sample = random.random() # return the next random floating point number in the range [0.0, 1.0]
    
    if sample > current_eps:
        with torch.no_grad():
            # t.max(1) will return largest column value of each row.
            # second column on max result is INDEX of where max element was
            # found, so we pick action with the larger expected reward.

            return policy_net(state).max(1)[1].view(1, 1)
    else:
        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)

"""# DQN Training Loop"""

# Architecture parameters and hyper-parameters
CAPACITY = 400000
NUM_EPISODES = 300
BATCH_SIZE = 128
TARGET_UPDATE = 3
LEARNING_RATE = 0.0075
eps_start = 0.9
eps_end = 0.05
eps_decay = 50
GAMMA = 0.999
FRAMES = 4

all_durations = []

for reps in range(10):
  durations = []
  print("doing the iteration number ", reps )
  # Creating the environment
  env = gym.make("CartPole-v1")

  # k-frames
  k = FRAMES

  # Get number of states and actions from gym action space
  env.reset()
  state_dim = k*len(env.state)    #FRAMES*(x, x_dot, theta, theta_dot)
  n_actions = env.action_space.n
  env.close()

  # Initialise other network parameters
  num_hidden_layers = 2
  size_hidden_layers = 40 

  # Create the policy network (predictor model)
  policy_net = DQN(state_dim, n_actions, num_hidden_layers, size_hidden_layers).to(device)

  # Create target network
  target_net = DQN(state_dim, n_actions, num_hidden_layers, size_hidden_layers).to(device)
  target_net.load_state_dict(policy_net.state_dict()) # Loading parameters on which we optimize policy_net into target_net
  target_net.eval()

  # Initialise experience replay memory
  memory = ReplayBuffer(CAPACITY) # argument is the capacity

  # Create optimiser
  optimizer = optim.RMSprop(policy_net.parameters(), lr = LEARNING_RATE) 

  
  continue_optimising = True
  duration_sub400_counter = 0
  duration_50_counter = 0
  duration = 0

  for tao in range(NUM_EPISODES): # for each episode/trace
    if tao % 20 == 0:
      print("episode ", tao, "/", NUM_EPISODES)

    epsilon = eps_end + (eps_start - eps_end) * math.exp(- tao/eps_decay) # epsilon decays as trace index increases

    # Initialize the environment and state
    state_list = []
    state = env.reset() 
    for i in range(k):
      state_list.extend(state) # initialise state to list of k times initial state - every time you choose a new state, append to this state var and remove the firt element (oldest state) 
    state = torch.tensor(state_list).float().unsqueeze(0).to(device)

    if duration > 400: # Track if model is doing good
      duration_500_counter += 1
    else:
      duration_500_counter = 0

    if tao > 100 and duration < 100: # Track if model is still doing bad after 100 episodes
      duration_50_counter += 1
    else:
      duration_50_counter = 0
    
    if duration_50_counter > 4: # If model was doing bad, restart optimising (or continue optimising) and increase exploration
      epsilon = 0.5
      continue_optimising = True 
    
    if duration_500_counter > 0: # I model is doing good, it has learnt, stop optimizing to avoid catastrophic forgetting
      continue_optimising = False
      print("5 500 durations reached - stop optimising!")
    
    if tao > 100 and duration < 400: 
      duration_sub400_counter += 1
    else:
      duration_sub400_counter = 0
    
    if duration_sub400_counter > 4: # If the model receives reward between 100 and 400 after 100 steps, restart optimizing (but don't increase exploration rate)
      continue_optimising = True  

    duration = 0
    for t in count():
      # Select and perform an action & observe new single state
      action = select_action(state, epsilon)
      one_next_state, reward, done, _ = env.step(action.item())
      reward = torch.tensor([reward], device=device) 
      
      # Generate new state with k frames
      state_list.extend(one_next_state)
      next_state = state_list[4*t+4:4*t+4*k+4]
      
      if not done:
        next_state = torch.tensor(next_state).float().unsqueeze(0).to(device)
      else:
        next_state = None
      
      # Store the transition in memory    
      memory.push(state, action, next_state, reward) 

      # Move to the next state
      state = next_state

      # Perform one step of the optimization (on the policy network) if we want to optimize the model
      # load_random_batches = torch.utils.data.DataLoader(memory, batch_size=BATCH_SIZE, shuffle = True, collate_fn=my_collate)
      if continue_optimising:
        optimize_model()

      duration += 1 

      if done: 
        break
    
    durations.append(duration)
    
    # Update target network with frequency TARGET_UPDATE
    if tao % TARGET_UPDATE == 0:
      target_net.load_state_dict(policy_net.state_dict()) #error could arise from the fact that we replaced object with Dataset 

  all_durations.append(durations) 
  # Save variable in drive 
  with open(PATH + "all_durations_noABL4.pickle", 'wb') as f:
    pickle.dump(all_durations, f)
  # Closing the environment
  env.close()

"""# DQN - Ablate Target Net"""

# Modified optimize_model function where we use the policy net to compute Q-learning targets
def optimize_model():
  if len(memory) < BATCH_SIZE:
    return
  transitions = memory.sample(BATCH_SIZE)
  batch = Transition(*zip(*transitions)) 
  
  # Compute a mask of non-final states and concatenate the batch elements
  non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,
                                        batch.next_state)), device=device, dtype=torch.bool) # tensor of boolean values (True = non-final state)
  if sum(non_final_mask) > 0:
    non_final_next_states = torch.cat([s for s in batch.next_state
                                                    if s is not None])
  else:
    non_final_next_states = torch.empty(0,state_dim).to(device)

  state_batch = torch.cat(batch.state) # tensor with all the states in the batch
  action_batch = torch.cat(batch.action) # tensor with all the actions in the batch
  reward_batch = torch.cat(batch.reward) # tensor with all the rewards in the batch
  
  # Compute Q(s_t, a) 
  state_action_values = policy_net(state_batch).gather(1, action_batch) # Compute Q for all possible actions on all states in the batch (state_batch) and choose the one for the action taken (with .gather())
  
  # Compute V(s_{t+1}) for all next states.
  next_state_values = torch.zeros(BATCH_SIZE, device=device)
  with torch.no_grad():
    if sum(non_final_mask) > 0:
      next_state_values[non_final_mask] = policy_net(non_final_next_states).max(1)[0].detach() # I want to select the action that give the max Q (return this Q). Detach() is to detach the gradient (don't want to store).
    else:
      next_state_values = torch.zeros_like(next_state_values)

  # Compute the expected Q values  
  expected_state_action_values = (next_state_values * GAMMA) + reward_batch

  # Compute loss
  loss = ((state_action_values - expected_state_action_values.unsqueeze(1))**2).sum()
  
  # Optimise the model
  optimizer.zero_grad()
  loss.backward()
  
  # Limit magnitude of gradient for update step
  for param in policy_net.parameters():
      param.grad.data.clamp_(-1, 1)
  optimizer.step()

# Hyper parameters stay the same, no need for TARGET_UPDATE
CAPACITY = 400000
NUM_EPISODES = 300
BATCH_SIZE = 128
# TARGET_UPDATE = 3
LEARNING_RATE = 0.0075
eps_start = 0.9
eps_end = 0.05
eps_decay = 50
GAMMA = 0.999
FRAMES = 4

all_durations = []

# Training Loop (is the same as above except that we do not initialise a target network)
for reps in range(10):
  durations = []
  print("doing the iteration number ", reps )
  # Creating the environment
  env = gym.make("CartPole-v1")
  # env = gym.wrappers.FrameStack(env,4)

  # k-frames
  k = FRAMES

  # Get number of states and actions from gym action space
  env.reset()
  state_dim = k*len(env.state)   
  n_actions = env.action_space.n
  env.close()

  # Initialise other network parameters
  num_hidden_layers = 2
  size_hidden_layers = 40 

  # Create the policy network (predictor model)
  policy_net = DQN(state_dim, n_actions, num_hidden_layers, size_hidden_layers).to(device)

  # Initialise experience replay memory
  memory = ReplayBuffer(CAPACITY) # argument is the capacity

  # Create optimiser
  optimizer = optim.RMSprop(policy_net.parameters(), lr = LEARNING_RATE) #1e-3

  continue_optimising = True
  duration = 0
  duration_sub400_counter = 0
  duration_50_counter = 0

  for tao in range(NUM_EPISODES):
    if tao % 20 == 0:
      print("episode ", tao, "/", NUM_EPISODES)

    epsilon = eps_end + (eps_start - eps_end) * math.exp(- tao/eps_decay)

    # Initialize the environment and state
    state_list = []
    state = env.reset() 
    for i in range(k):
      state_list.extend(state) 
    state = torch.tensor(state_list).float().unsqueeze(0).to(device) 

    if duration > 400:
      duration_500_counter += 1
    else:
      duration_500_counter = 0

    if tao > 100 and duration < 100:
      duration_50_counter += 1
    else:
      duration_50_counter = 0
    
    if duration_50_counter > 4:
      epsilon = 0.5
      continue_optimising = True 
    
    if duration_500_counter > 0:
      continue_optimising = False
      print("5 500 durations reached - stop optimising!")
    
    if tao > 100 and duration < 400:
      duration_sub400_counter += 1
    else:
      duration_sub400_counter = 0
    
    if duration_sub400_counter > 4:
      continue_optimising = True 

    duration = 0
    for t in count():
      # Select and perform an action & observe new single state
      action = select_action(state, epsilon)
      one_next_state, reward, done, _ = env.step(action.item())
      reward = torch.tensor([reward], device=device) 
      
      # Generate new state with k frames
      state_list.extend(one_next_state)
      next_state = state_list[4*t+4:4*t+4*k+4]
      
      if not done:
        next_state = torch.tensor(next_state).float().unsqueeze(0).to(device)
      else:
        next_state = None
      
      # Store the transition in memory    
      memory.push(state, action, next_state, reward) 

      # Move to the next state
      state = next_state

      # Perform one step of the optimization (on the policy network)
      # load_random_batches = torch.utils.data.DataLoader(memory, batch_size=BATCH_SIZE, shuffle = True, collate_fn=my_collate)
      if continue_optimising:
        optimize_model()

      duration += 1 

      if done: 
        break
    
    durations.append(duration)
    
  all_durations.append(durations)  
  with open(PATH + "all_durations_targABL6.pickle", 'wb') as f:
    pickle.dump(all_durations, f)
  # Closing the environment
  env.close()

"""# DQN - Ablate Replay Buffer"""

# Back to original optimize_model() function
def optimize_model():
  if len(memory) < BATCH_SIZE:
    return
  transitions = memory.sample(BATCH_SIZE)
  batch = Transition(*zip(*transitions)) 
  
  # Compute a mask of non-final states and concatenate the batch elements
  non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,
                                        batch.next_state)), device=device, dtype=torch.bool) # tensor of boolean values (True = non-final state)
  if sum(non_final_mask) > 0:
    non_final_next_states = torch.cat([s for s in batch.next_state
                                                    if s is not None])
  else:
    non_final_next_states = torch.empty(0,state_dim).to(device)

  state_batch = torch.cat(batch.state) # tensor with all the states in the batch
  action_batch = torch.cat(batch.action) # tensor with all the actions in the batch
  reward_batch = torch.cat(batch.reward) # tensor with all the rewards in the batch
  
  # Compute Q(s_t, a) 
  state_action_values = policy_net(state_batch).gather(1, action_batch) # Compute Q for all possible actions on all states in the batch (state_batch) and choose the one for the action taken (with .gather())
  
  # Compute V(s_{t+1}) for all next states.
  next_state_values = torch.zeros(BATCH_SIZE, device=device)
  with torch.no_grad():
    if sum(non_final_mask) > 0:
      next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach() # I want to select the action that give the max Q (return this Q). Detach() is to detach the gradient (don't want to store).
    else:
      next_state_values = torch.zeros_like(next_state_values)

  # Compute the expected Q values  
  expected_state_action_values = (next_state_values * GAMMA) + reward_batch

  # Compute loss
  loss = ((state_action_values - expected_state_action_values.unsqueeze(1))**2).sum()
  
  # Optimise the model
  optimizer.zero_grad()
  loss.backward()
  
  # Limit magnitude of gradient for update step
  for param in policy_net.parameters():
      param.grad.data.clamp_(-1, 1)
  optimizer.step()

# Ablate memory buffer: set CAPACITY to 1 (entails BATCH_SIZE = 1)
CAPACITY = 1
NUM_EPISODES = 300
BATCH_SIZE = 1
TARGET_UPDATE = 3
LEARNING_RATE = 0.0075
eps_start = 0.9
eps_end = 0.05
eps_decay = 50
GAMMA = 0.999
FRAMES = 4

all_durations = []

# Same training loop as DQN 
for reps in range(10):
  durations = []
  print("doing the iteration number ", reps )
  # Creating the environment
  env = gym.make("CartPole-v1")
  # env = gym.wrappers.FrameStack(env,4)

  # k-frames
  k = FRAMES

  # Get number of states and actions from gym action space
  env.reset()
  state_dim = k*len(env.state)    
  n_actions = env.action_space.n
  env.close()

  # Initialise other network parameters
  num_hidden_layers = 2
  size_hidden_layers = 40 

  # Create the policy network (predictor model)
  policy_net = DQN(state_dim, n_actions, num_hidden_layers, size_hidden_layers).to(device)

  # Create target network
  target_net = DQN(state_dim, n_actions, num_hidden_layers, size_hidden_layers).to(device)
  target_net.load_state_dict(policy_net.state_dict())
  target_net.eval()

  # Initialise experience replay memory
  memory = ReplayBuffer(CAPACITY) # argument is the capacity

  # Create optimiser
  optimizer = optim.RMSprop(policy_net.parameters(), lr = LEARNING_RATE) 

  continue_optimising = True
  duration = 0
  duration_sub400_counter = 0
  duration_50_counter = 0

  for tao in range(NUM_EPISODES):
    if tao % 20 == 0:
      print("episode ", tao, "/", NUM_EPISODES)

    epsilon = eps_end + (eps_start - eps_end) * math.exp(- tao/eps_decay)

    # Initialize the environment and state
    state_list = []
    state = env.reset() 
    for i in range(k):
      state_list.extend(state) 
    state = torch.tensor(state_list).float().unsqueeze(0).to(device) 

    if duration > 400:
      duration_500_counter += 1
    else:
      duration_500_counter = 0

    if tao > 100 and duration < 100:
      duration_50_counter += 1
    else:
      duration_50_counter = 0
    
    if duration_50_counter > 4:
      epsilon = 0.5
      continue_optimising = True #??
    
    if duration_500_counter > 0:
      continue_optimising = False
      print("5 500 durations reached - stop optimising!")
    
    if tao > 100 and duration < 400:
      duration_sub400_counter += 1
    else:
      duration_sub400_counter = 0
    
    if duration_sub400_counter > 4:
      continue_optimising = True 

    duration = 0
    for t in count():
      # Select and perform an action & observe new single state
      action = select_action(state, epsilon)
      one_next_state, reward, done, _ = env.step(action.item())
      # print("one_next_state", one_next_state)
      reward = torch.tensor([reward], device=device) 
      
      # Generate new state with k frames
      state_list.extend(one_next_state)
      # print("state_list", state_list)
      # print("state list with next state", state_list)
      next_state = state_list[4*t+4:4*t+4*k+4]
      # print("next state list", next_state)
      
      if not done:
        next_state = torch.tensor(next_state).float().unsqueeze(0).to(device)
      else:
        next_state = None
      
      # Store the transition in memory    
      memory.push(state, action, next_state, reward) 

      # Move to the next state
      state = next_state

      # Perform one step of the optimization (on the policy network)
      # load_random_batches = torch.utils.data.DataLoader(memory, batch_size=BATCH_SIZE, shuffle = True, collate_fn=my_collate)
      if continue_optimising:
        optimize_model()

      duration += 1 

      if done: 
        break
    
    durations.append(duration)
    

    if tao % TARGET_UPDATE == 0:
      target_net.load_state_dict(policy_net.state_dict()) #error could arise from the fact that we replaced object with Dataset 

  all_durations.append(durations)  
  with open(PATH + "all_durations_rbABL6.pickle", 'wb') as f:
    pickle.dump(all_durations, f)
  # Closing the environment
  env.close()

"""# Q2.1 Epsilons"""

# Rerun the "functions anf classes (multipurpose) cell"

CAPACITY = 400000
NUM_EPISODES = 300
BATCH_SIZE = 128
TARGET_UPDATE = 3
LEARNING_RATE = 0.0075
eps_start = 0.9
eps_end = 0.05
eps_decay = 50
GAMMA = 0.999
FRAMES = 4

all_durations = []

not_constant = [0,    0,    0,    0,    1,    1,    1,    1] # If not_constant is false epsilon IS constant
eps_start =    [0,    0.3,  0.7,  1,    1,    1,    1,    0.5]
eps_end =      [0,    0,    0,    0,    0.1, 0.1, 0.1, 0.1]
eps_decay =    [100,  100,  100,  100,  50,  100,  200,  100]

# Same training loop as DQN but for different epsilons (only modification in the line whare epsilon is defined)
# TESTS FOR DIFFERENT EPSILONS
durations_across_eps_and_reps = []
for eps in range(10):
  print(eps)
  durations_across_reps = []
  for reps in range(10):
    avg_losses = []
    durations = []
    print("doing the iteration number ", reps )
    # Creating the environment
    env = gym.make("CartPole-v1")

    # k-frames
    k = FRAMES

    # Get number of states and actions from gym action space
    env.reset()
    state_dim = k*len(env.state)    
    n_actions = env.action_space.n
    env.close()

    # Initialise other network parameters
    num_hidden_layers = 2
    size_hidden_layers = 40 

    # Create the policy network (predictor model)
    policy_net = DQN(state_dim, n_actions, num_hidden_layers, size_hidden_layers).to(device)

    # Create target network
    target_net = DQN(state_dim, n_actions, num_hidden_layers, size_hidden_layers).to(device)
    target_net.load_state_dict(policy_net.state_dict()) 
    target_net.eval()

    # Initialise experience replay memory
    memory = ReplayBuffer(CAPACITY) # argument is the capacity

    # Create optimiser
    optimizer = optim.RMSprop(policy_net.parameters(), lr = LEARNING_RATE) 

    continue_optimising = True
    duration = 0
    duration_sub400_counter = 0
    duration_50_counter = 0
    
    for tao in range(NUM_EPISODES):
      if tao % 20 == 0:
        print("episode ", tao, "/", NUM_EPISODES)

      epsilon = eps_end[eps] + (eps_start[eps] - eps_end[eps]) * math.exp(- tao*not_constant[eps]/eps_decay[eps])


      # Initialize the environment and state
      state_list = []
      state = env.reset() # initialise state to list of 4 times initial state - every time you choose a new state, append to this state var and remove the firt element (oldest state) 
      for i in range(k):
        state_list.extend(state) 
      state = torch.tensor(state_list).float().unsqueeze(0).to(device) 

      
      if duration > 400:
        duration_500_counter += 1
      else:
        duration_500_counter = 0

      if tao > 100 and duration < 100:
        duration_50_counter += 1
      else:
        duration_50_counter = 0
      
      if duration_50_counter > 4:
        epsilon = 0.5
        continue_optimising = True 
      
      if duration_500_counter > 0:
        continue_optimising = False
      
      if tao > 100 and duration < 400:
        duration_sub400_counter += 1
      else:
        duration_sub400_counter = 0
      
      if duration_sub400_counter > 4:
        continue_optimising = True 

      duration = 0

      for t in count():
        # Select and perform an action
        action = select_action(state, epsilon)

        # Observe new state 
        one_next_state, reward, done, _ = env.step(action.item())
        # print("one_next_state", one_next_state)
        reward = torch.tensor([reward], device=device) 
        
        # Generate new state with k frames
        state_list.extend(one_next_state)
        # print("state_list", state_list)
        # print("state list with next state", state_list)
        next_state = state_list[4*t+4:4*t+4+4*k]
        # print("next state list", next_state)
        
        if not done:
          next_state = torch.tensor(next_state).float().unsqueeze(0).to(device)
        else:
          next_state = None
        
        # Store the transition in memory    
        memory.push(state, action, next_state, reward) 

        # Move to the next state
        state = next_state

        # Perform one step of the optimization (on the policy network)
        # load_random_batches = torch.utils.data.DataLoader(memory, batch_size=BATCH_SIZE, shuffle = True, collate_fn=my_collate)
        if continue_optimising:
          optimize_model()

        duration += 1 

        if done: 
          break
      
      # avg_losses.append(sum(all_losses)/duration)
      durations.append(duration)
      

      if tao % TARGET_UPDATE == 0:
        target_net.load_state_dict(policy_net.state_dict()) #error could arise from the fact that we replaced object with Dataset 

    durations_across_reps.append(durations)  
    # Closing the environment
    env.close()

  durations_across_eps_and_reps.append(durations_across_reps) # maybe just already store mean and std dev? 
  with open(PATH + "durations_across_eps_and_reps4.pickle", 'wb') as f:
    pickle.dump(durations_across_eps_and_reps, f)

"""# Q2.2 Capacity/Buffer Size"""

CAPACITY_TEST = [1, 10, 100, 1000, 10000, 100000]

# Similar training loop as for different epsilons but we change the capacity and not the epsilon (decay as in DQN implementation)

# Code to prep plot:
file = open(PATH + "durations_across_cap_and_reps.pickle",'rb')
durations_across_cap_and_reps = pickle.load(file)
mean_durations= np.mean(durations_across_cap_and_reps, axis = 1)
std_durations= np.std(durations_across_cap_and_reps, axis = 1)

maxes = np.max(mean_durations, axis = 1)
two3rds_maxes = (2/3)*maxes

episode_for_std = []
for caps in range(len(CAPACITY_TEST)):
  for i in range(len(mean_durations[0])):
    if mean_durations[caps][i] > two3rds_maxes[caps]:
      episode_for_std.append(i)
      break

std_plot = []
for caps in range(len(CAPACITY_TEST)):
  std_plot.append(std_durations[caps][episode_for_std[caps]])

# Plot
import seaborn as sns
sns.set_style("whitegrid")
plt.plot(CAPACITY_TEST, std_plot)
plt.xscale("log")
plt.xlabel("Buffer Size (log scale)", fontsize=13.5)
plt.ylabel("Standard Deviation", fontsize=13.5)

"""#Q2.3 K-frames"""

# Similar training loop as for different epsilons but we change k and not the epsilon (decay as in DQN implementation)

FRAMES_TEST = [1, 2, 3, 4, 10]

"""# DDQN"""

# Modified optimize_model() function to implement DDQN - changes where we compute next_state_values
def optimize_model():
  if len(memory) < BATCH_SIZE:
    return
  transitions = memory.sample(BATCH_SIZE)
  batch = Transition(*zip(*transitions)) 
  
  # Compute a mask of non-final states and concatenate the batch elements
  non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,
                                        batch.next_state)), device=device, dtype=torch.bool) # tensor of boolean values (True = non-final state)
  if sum(non_final_mask) > 0:
    non_final_next_states = torch.cat([s for s in batch.next_state
                                                    if s is not None])
  else:
    non_final_next_states = torch.empty(0,state_dim).to(device)

  state_batch = torch.cat(batch.state) # tensor with all the states in the batch
  action_batch = torch.cat(batch.action) # tensor with all the actions in the batch
  reward_batch = torch.cat(batch.reward) # tensor with all the rewards in the batch
  
  # Compute Q(s_t, a) 
  state_action_values = policy_net(state_batch).gather(1, action_batch) # Compute Q for all possible actions on all states in the batch (state_batch) and choose the one for the action taken (with .gather())
  
  # Compute V(s_{t+1}) for all next states.
  next_state_values = torch.zeros(BATCH_SIZE, device=device)
  with torch.no_grad():
    if sum(non_final_mask) > 0:
      max_act = policy_net(non_final_next_states).argmax(1) # max across axis = 1 (in each row), i.e. the index of the max action from action values outputed by the policy_net, for each non_final_next_state
      next_state_values[non_final_mask] = target_net(non_final_next_states).gather(1, max_act.view(-1,1)).squeeze(1).detach() # Value of max action computed by the target_net
    else:
      next_state_values = torch.zeros_like(next_state_values)

  # Compute the expected Q values  
  expected_state_action_values = (next_state_values * GAMMA) + reward_batch

  # Compute loss
  loss = ((state_action_values - expected_state_action_values.unsqueeze(1))**2).sum()
  
  # Optimise the model
  optimizer.zero_grad()
  loss.backward()
  
  # Limit magnitude of gradient for update step
  for param in policy_net.parameters():
      param.grad.data.clamp_(-1, 1)
  optimizer.step()

# Same parameters as the DQN
CAPACITY = 400000
NUM_EPISODES = 300
BATCH_SIZE = 128
TARGET_UPDATE = 3
LEARNING_RATE = 0.0075
eps_start = 0.9
eps_end = 0.05
eps_decay = 50
GAMMA = 0.999
FRAMES = 4

all_durations = []

# Same training loop as DQN (only change is in the optimize_model() function)
for reps in range(10):
  durations = []
  print("doing the iteration number ", reps )
  # Creating the environment
  env = gym.make("CartPole-v1")
  # env = gym.wrappers.FrameStack(env,4)

  # k-frames
  k = FRAMES

  # Get number of states and actions from gym action space
  env.reset()
  state_dim = k*len(env.state)    #x, x_dot, theta, theta_dot # state dim is now 4*4
  n_actions = env.action_space.n
  env.close()

  # Initialise other network parameters
  num_hidden_layers = 2
  size_hidden_layers = 40 # 15

  # Create the policy network (predictor model)
  policy_net = DQN(state_dim, n_actions, num_hidden_layers, size_hidden_layers).to(device)

  # Create target network
  target_net = DQN(state_dim, n_actions, num_hidden_layers, size_hidden_layers).to(device)
  target_net.load_state_dict(policy_net.state_dict()) # comment!!!
  target_net.eval()

  # Initialise experience replay memory
  memory = ReplayBuffer(CAPACITY) # argument is the capacity

  # Create optimiser
  optimizer = optim.RMSprop(policy_net.parameters(), lr = LEARNING_RATE) #1e-3

  continue_optimising = True
  duration = 0
  duration_sub400_counter = 0
  duration_50_counter = 0
  
  for tao in range(NUM_EPISODES):
    if tao % 20 == 0:
      print("episode ", tao, "/", NUM_EPISODES)

    epsilon = eps_end + (eps_start - eps_end) * math.exp(- tao/eps_decay)

    # Initialize the environment and state
    state_list = []
    state = env.reset() # initialise state to list of 4 times initial state - every time you choose a new state, append to this state var and remove the firt element (oldest state) 
    for i in range(k):
      state_list.extend(state) 
    state = torch.tensor(state_list).float().unsqueeze(0).to(device)

    if duration > 400:
      duration_500_counter += 1
    else:
      duration_500_counter = 0

    if tao > 100 and duration < 100:
      duration_50_counter += 1
    else:
      duration_50_counter = 0
    
    if duration_50_counter > 4:
      epsilon = 0.5
      continue_optimising = True #??
    
    if duration_500_counter > 0:
      continue_optimising = False
      print("5 500 durations reached - stop optimising!")
    
    if tao > 100 and duration < 400:
      duration_sub400_counter += 1
    else:
      duration_sub400_counter = 0
    
    if duration_sub400_counter > 4:
      continue_optimising = True #?? 

    duration = 0
    for t in count():
      # Select and perform an action & observe new single state
      action = select_action(state, epsilon)
      one_next_state, reward, done, _ = env.step(action.item())
      # print("one_next_state", one_next_state)
      reward = torch.tensor([reward], device=device) 
      
      # Generate new state with k frames
      state_list.extend(one_next_state)
      # print("state_list", state_list)
      # print("state list with next state", state_list)
      next_state = state_list[4*t+4:4*t+4*k+4]
      # print("next state list", next_state)
      
      if not done:
        next_state = torch.tensor(next_state).float().unsqueeze(0).to(device)
      else:
        next_state = None
      
      # Store the transition in memory    
      memory.push(state, action, next_state, reward) 

      # Move to the next state
      state = next_state

      # Perform one step of the optimization (on the policy network)
      # load_random_batches = torch.utils.data.DataLoader(memory, batch_size=BATCH_SIZE, shuffle = True, collate_fn=my_collate)
      if continue_optimising:
        optimize_model()


      duration += 1 

      if done: 
        break
    
    durations.append(duration)
    

    if tao % TARGET_UPDATE == 0:
      target_net.load_state_dict(policy_net.state_dict()) #error could arise from the fact that we replaced object with Dataset 

  all_durations.append(durations)  
  with open(PATH + "all_durations_DDQN.pickle", 'wb') as f:
    pickle.dump(all_durations, f)
  # Closing the environment
  env.close()

"""# Plots for Q1 and Q3"""

from google.colab import drive
drive.mount('/content/gdrive')
PATH = 'gdrive/MyDrive/Reinforcement Learning/'
PATH = 'gdrive/MyDrive/Colab Notebooks/Livia/'

import pickle

file = open(PATH + "all_durations_noABL4.pickle",'rb')
all_durations_noABL = pickle.load(file)

file = open(PATH + "all_durations_targABL5.pickle",'rb')
all_durations_targABL = pickle.load(file)

file = open(PATH + "all_durations_rbABL4.pickle",'rb')
all_durations_rbABL = pickle.load(file)

file = open(PATH + "all_durations_DDQN.pickle",'rb')
all_durations_DDQN = pickle.load(file)

mean_durations = np.mean(all_durations_noABL, axis=0)
std_durations = np.std(all_durations_noABL, axis=0)

mean_durationsTARG = np.mean(all_durations_targABL, axis=0)
std_durationsTARG = np.std(all_durations_targABL, axis=0)

mean_durationsRB = np.mean(all_durations_rbABL, axis=0)
std_durationsRB = np.std(all_durations_rbABL, axis=0)

mean_durationsDDQN = np.mean(all_durations_DDQN, axis=0)
std_durationsDDQN = np.std(all_durations_DDQN, axis=0)

# Q3 MEAN LEARNING CURVES
import seaborn as sns
sns.set_style("whitegrid")
# Visualize the result
plt.plot(range(NUM_EPISODES), mean_durations, '-', label = "DQN")
plt.plot(range(NUM_EPISODES), mean_durationsTARG, '-', label = "No Target Network")
plt.plot(range(NUM_EPISODES), mean_durationsRB, '-', label = "No Replay Buffer")
plt.plot(range(NUM_EPISODES), mean_durationsDDQN, '-', label = "DDQN")

plt.xlim(0, NUM_EPISODES);
plt.legend()
plt.xlabel("Episodes", fontsize=13.5)
plt.ylabel("Mean total undiscounted return", fontsize=13.5)

# Q3 STD LEARNING CURVES
import seaborn as sns
sns.set_style("whitegrid")
# Visualize the result
plt.plot(range(NUM_EPISODES), std_durations, '-', label = "DQN")
plt.plot(range(NUM_EPISODES), std_durationsTARG, '-', label = "No Target Network")
plt.plot(range(NUM_EPISODES), std_durationsRB, '-', label = "No Replay Buffer")
plt.plot(range(NUM_EPISODES), std_durationsDDQN, '-', label = "DDQN")

plt.xlim(0, NUM_EPISODES);
plt.legend()
plt.xlabel("Episodes", fontsize=13.5)
plt.ylabel("Std total undiscounted return", fontsize=13.5)

# Q1 MEAN and STD of LEARNING CURVE
import seaborn as sns
sns.set_style("whitegrid")
# Visualize the result
plt.plot(range(NUM_EPISODES), mean_durations, '-', label = "DQN")
plt.fill_between(range(NUM_EPISODES), mean_durations - std_durations, mean_durations + std_durations,
                 alpha=0.3)

plt.xlim(0, NUM_EPISODES);

plt.xlabel("Episodes", fontsize=13.5)
plt.ylabel("Mean total undiscounted return", fontsize=13.5)

"""# Assess Outcome (with video)"""

## run an episode with trained agent and record video
## remember to change file_path name if you do not wish to overwrite an existing video

env = gym.make("CartPole-v1")
file_path = 'video/video5.mp4'
recorder = VideoRecorder(env, file_path)

observation = env.reset()
done = False

state = state = torch.tensor(env.state).float().unsqueeze(0)

duration = 0

while not done:
    recorder.capture_frame()

    # Select and perform an action
    print(policy_net(state))
    action = select_action(state)
    observation, reward, done, _ = env.step(action.item())
    duration += 1
    reward = torch.tensor([reward], device=device)

    # Observe new state
    state = torch.tensor(env.state).float().unsqueeze(0)

recorder.close()
env.close()
print("Episode duration: ", duration)

## run an episode with trained agent and record video
## remember to change file_path name if you do not wish to overwrite an existing video

k = 4

env = gym.make("CartPole-v1")

state_list = []
state = env.reset() # initialise state to list of 4 times initial state - every time you choose a new state, append to this state var and remove the firt element (oldest state) 
for i in range(k):
  state_list.extend(state) 
state = torch.tensor(state_list).float().unsqueeze(0).to(device) 

done = False
duration = 0

while not done:

    # Select and perform an action
    print(state)
    print(policy_net(state))
    action = select_action(state)
    # Observe new state 
    one_next_state, reward, done, _ = env.step(action.item())
    reward = torch.tensor([reward], device=device) 
    

    state_list.extend(one_next_state)
    next_state = state_list[4*duration+4:4*duration+4+4*k]
    next_state = torch.tensor(next_state).float().unsqueeze(0).to(device)

    duration += 1

    print(reward)
    print("done", done)

    # Observe new state
    state = next_state


env.close()
print("Episode duration: ", duration)